{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "\n",
    "attributes = [\"VoW\", \"SoW\", \"CoW\", \"Entropy\", \"Label\"]\n",
    "bank_note_training = pd.read_csv(\"https://raw.githubusercontent.com/DoubekSeth/ToyDatasets/main/bank-note/train.csv\", header=None, names=attributes)\n",
    "bank_note_testing = pd.read_csv(\"https://raw.githubusercontent.com/DoubekSeth/ToyDatasets/main/bank-note/test.csv\", header=None, names=attributes)\n",
    "\n",
    "def add_bias_term_to_df(df):\n",
    "    rows = df.shape[0]\n",
    "    columns = df.shape[1]\n",
    "    ones = np.ones(rows)\n",
    "    #Need to check if actually need to insert\n",
    "    if(\"Bias Term\" not in df.columns):\n",
    "        df.insert(columns-1, \"Bias Term\", ones, False)\n",
    "\n",
    "\n",
    "def dualSVM(training, kernel, C):\n",
    "    #Add bias term to dataframe\n",
    "    add_bias_term_to_df(training)\n",
    "    #Initialize weights to 0\n",
    "    y_vec = 2*training[\"Label\"]-1\n",
    "    #For training examples, find optimal values\n",
    "    kernelMatrix = np.fromfunction(np.vectorize(lambda i, j: kernel(training.iloc[int(i), :training.shape[1]-1], training.iloc[int(j), :training.shape[1]-1])), (training.shape[0], training.shape[0]))\n",
    "\n",
    "    def objective_function(X):\n",
    "        return 0.5*np.sum(np.dot(kernelMatrix * np.array((y_vec * X))[:, np.newaxis], y_vec*X))-np.sum(X)\n",
    "\n",
    "    bounds = optimize.Bounds(lb=0, ub=C)\n",
    "\n",
    "    def mutualSlack(X):\n",
    "        return np.dot(X, y_vec)-0\n",
    "        \n",
    "    constraint_dict = {'type': 'eq', 'fun':mutualSlack}\n",
    "\n",
    "    initial_guess = np.zeros(len(y_vec))\n",
    "\n",
    "    result = optimize.minimize(objective_function, initial_guess, method=\"SLSQP\", constraints=constraint_dict, bounds=bounds)[\"x\"]\n",
    "    #Set all alphas close to 0 to 0\n",
    "    epsilon = 0.00000001\n",
    "    result[result < epsilon] = 0\n",
    "    return result\n",
    "\n",
    "def noKernel(a, b):\n",
    "    return np.dot(a, b)\n",
    "\n",
    "def gaussiankernel(a, b, c):\n",
    "    return np.e**(-(np.linalg.norm(a-b)**2)/c)\n",
    "\n",
    "def evaluate_dualSVM(testing, training, alphas, kernel):\n",
    "    correct = 0\n",
    "    add_bias_term_to_df(testing)\n",
    "    add_bias_term_to_df(training)\n",
    "    test_y = 2*testing[\"Label\"]-1\n",
    "    #Create kernel matrix\n",
    "    kernelMatrix = np.fromfunction(np.vectorize(lambda i, j: kernel(training.iloc[int(i), :training.shape[1]-1], testing.iloc[int(j), :testing.shape[1]-1])), (training.shape[0], testing.shape[0]))\n",
    "    alphayis = alphas*(2*training[\"Label\"]-1)\n",
    "    vecsgn = np.vectorize(sgn)\n",
    "    predictions = vecsgn(np.dot(alphayis, kernelMatrix))\n",
    "    errs = 0.5*np.sum(np.abs(predictions-test_y))\n",
    "    return (testing.shape[0]-errs)/testing.shape[0]\n",
    "\n",
    "def sgn(x):\n",
    "    return 1 if x >= 0 else -1\n",
    "\n",
    "def recover_weights(alphas, training):\n",
    "    y_vec = 2*training[\"Label\"]-1\n",
    "    x_vec = training[[\"VoW\", \"SoW\", \"CoW\", \"Entropy\", \"Bias Term\"]]\n",
    "    w = np.sum(x_vec * np.array((y_vec * alphas))[:, np.newaxis], axis=0)\n",
    "    return w\n",
    "\n",
    "# for i in range(0, 3):\n",
    "#     if(i == 0):\n",
    "#         C = 100/873\n",
    "#     if(i == 1):\n",
    "#         C = 500/873\n",
    "#     if(i == 2):\n",
    "#         C = 700/873\n",
    "#     alphas = dualSVM(bank_note_training, noKernel, C=C)\n",
    "#     print(evaluate_dualSVM(testing=bank_note_testing, training=bank_note_training, alphas=alphas, kernel=noKernel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = dualSVM(bank_note_training, noKernel, C=700/873)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.80183276 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.59910759 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.80183276 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.80183276 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.28041875 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.80183276 0.         0.         0.         0.80183276\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.80183276 0.         0.         0.\n",
      " 0.80183276 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.80183276 0.         0.         0.         0.16916874\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.80183276\n",
      " 0.         0.         0.80183276 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.80183276 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.80183276 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.80183276 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.37154669 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.80183276 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.80183276 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.49627403 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.80183276 0.         0.80183276 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.80183276 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.80183276\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.80183276\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.80183276 0.80183276\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.80183276 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.80183276 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.37158965 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.80183276\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.        ]\n",
      "Number of support vectors 31\n"
     ]
    }
   ],
   "source": [
    "print(alphas)\n",
    "#print(evaluate_dualSVM(testing=bank_note_testing, training=bank_note_training, alphas=alphas, kernel=noKernel))\n",
    "#print(recover_weights(alphas=alphas, training=bank_note_training))\n",
    "print(\"Number of support vectors\", np.count_nonzero(alphas))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
