{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_header = [\"Cement\", \"Slag\", \"Fly Ash\", \"Water\", \"SP\", \"Coarse Aggr\", \"Fine Aggr\", \"SLUMP Flow\"]\n",
    "train_concrete = pd.read_csv(\"https://raw.githubusercontent.com/DoubekSeth/ToyDatasets/main/concrete/train.csv\", names=concrete_header)\n",
    "test_concrete = pd.read_csv(\"https://raw.githubusercontent.com/DoubekSeth/ToyDatasets/main/concrete/test.csv\", names=concrete_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method that adds ones to a dataframe, makes algorithm simplier to implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addOnesForBiasOnDataset(df):\n",
    "    rows = df.shape[0]\n",
    "    columns = df.shape[1]\n",
    "    ones = np.ones(rows)\n",
    "    #Need to check if actually need to insert\n",
    "    if(\"Bias Term\" not in df.columns):\n",
    "        df.insert(columns-1, \"Bias Term\", ones, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchGD(train_df, gradient_of_cost_func, r, weights, convergence):\n",
    "    converged = False\n",
    "    t=0\n",
    "    costs=[]\n",
    "    while(not converged):\n",
    "        grad = gradient_of_cost_func(train_df, weights)\n",
    "        #print(grad)\n",
    "        new_weights = weights - r*grad\n",
    "        if(abs(np.sum(weights-new_weights)) < convergence):\n",
    "            converged=True\n",
    "        weights = new_weights\n",
    "        #Print cost\n",
    "        cost = cost_MSE_df(weights, train_df)\n",
    "        t+=1\n",
    "        print(\"Cost at step\", t, \":\", cost)\n",
    "        costs.append(cost)\n",
    "    return weights, costs, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the gradient of the training data with the weights for a MSE cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_Batch_MSE(train_df, weights):\n",
    "    grad = np.zeros(train_df.shape[1]-1) #Subtract one for the label\n",
    "    X = train_df.drop(\"SLUMP Flow\", axis=1)\n",
    "    Y = train_df[\"SLUMP Flow\"]\n",
    "    for i in range(len(grad)):\n",
    "        X_i = train_df.iloc[:, i]\n",
    "        #print(\"WTX\", np.dot(X, weights))\n",
    "        #print(\"Y-WTX\", Y-np.dot(X, weights))\n",
    "        #print(\"X_i\", X_i)\n",
    "        #print(\"-(Y-WTX)X_i\", -np.dot((Y-np.dot(X, weights)), X_i))\n",
    "        grad[i] = -np.dot((Y-np.dot(X, weights)), X_i)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StochasticGD(train_df, r, weights, convergence):\n",
    "    converged = False\n",
    "    t=0\n",
    "    costs=[]\n",
    "    while(not converged):\n",
    "        for index, example in train_df.iterrows():\n",
    "            new_weights = copy.copy(weights)\n",
    "            for j in range(len(weights)):\n",
    "                #print(train_df.iloc[index, train_df.shape[1]-1])\n",
    "                #print(np.dot(example.drop(\"SLUMP Flow\"), weights))\n",
    "                #print(train_df.iloc[index, j])\n",
    "                new_weights[j] = weights[j] + r*(train_df.iloc[index, train_df.shape[1]-1]-np.dot(example.drop(\"SLUMP Flow\"), weights)*train_df.iloc[index, j])            \n",
    "            if(abs(np.sum(weights-new_weights)) < convergence):\n",
    "                converged=True\n",
    "            weights=new_weights\n",
    "            t+=1\n",
    "            cost = cost_MSE_df(weights, train_df)\n",
    "            print(\"Cost at step\", t, \":\", cost)\n",
    "            costs.append(cost)\n",
    "    return weights, costs, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_MSE_df(weights, data):\n",
    "    X = data.drop(\"SLUMP Flow\", axis=1)\n",
    "    Y = data[\"SLUMP Flow\"]\n",
    "    return 0.5*(np.sum(np.square(Y-np.dot(X, weights))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_MSE(weights, datum):\n",
    "    return 0.5*(datum[\"SLUMP Flow\"]-np.dot(weights, datum.drop(\"SLUMP Flow\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights = np.zeros(8)\n",
    "addOnesForBiasOnDataset(train_concrete)\n",
    "#print(gradient_Batch_MSE(train_concrete, weights))\n",
    "weights, costs, steps = BatchGD(train_concrete, gradient_Batch_MSE, 0.005, weights, .000001)\n",
    "plt.plot(np.linspace(1, steps, steps), costs, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.grid(True)\n",
    "plt.show\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "addOnesForBiasOnDataset(test_concrete)\n",
    "cost_MSE_df(weights, test_concrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights = np.zeros(8)\n",
    "addOnesForBiasOnDataset(train_concrete)\n",
    "#print(gradient_Batch_MSE(train_concrete, weights))\n",
    "weights, costs, steps = StochasticGD(train_concrete, 0.02, weights, .00002)\n",
    "plt.plot(np.linspace(1, steps, steps), costs, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.grid(True)\n",
    "plt.show\n",
    "print(weights)\n",
    "addOnesForBiasOnDataset(test_concrete)\n",
    "print(cost_MSE_df(weights, test_concrete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights, residuals, rank, s = np.linalg.lstsq(train_concrete.drop(\"SLUMP Flow\", axis=1), train_concrete[\"SLUMP Flow\"])\n",
    "print(weights)\n",
    "cost_MSE_df(weights, test_concrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper_df = pd.DataFrame([[1, -1, 2, 1], [1, 1, 3, 4], [-1, 1, 0, -1], [1, 2, -4, -2], [3, -1, -1, 0]])\n",
    "paper_df.columns = [\"x_1\", \"x_2\", \"x_3\", \"SLUMP Flow\"]\n",
    "addOnesForBiasOnDataset(paper_df)\n",
    "print(paper_df)\n",
    "\n",
    "StochasticGD(paper_df, r=0.1, weights=np.zeros(4), convergence=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
